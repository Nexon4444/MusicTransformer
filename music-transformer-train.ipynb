{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "music-transformer-train.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install progress\n",
    "!pip install pretty_midi\n",
    "!pip install wandb"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kDWpEd95_NyB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_17936/557506142.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mdrive\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'/content/drive/'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Sll3ovb_Nx4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490342022,
     "user_tz": -60,
     "elapsed": 2658,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "4a7b5c4b-89eb-4021-e77e-d69003b07ff5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/My Drive/MusicTransformer\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/'My Drive'/MusicTransformer\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hq70x3Ds_Nx8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490367295,
     "user_tz": -60,
     "elapsed": 256,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "4ac20099-382a-4094-fcc3-6f59784aac39"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 00B4-5FB3\n",
      "\n",
      " Directory of C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\n",
      "\n",
      "16.12.2021  16:20    <DIR>          .\n",
      "16.12.2021  16:20    <DIR>          ..\n",
      "16.12.2021  12:15               114 .gitignore\n",
      "16.12.2021  16:20    <DIR>          .idea\n",
      "16.12.2021  15:04    <DIR>          __pycache__\n",
      "16.12.2021  12:39    <DIR>          custom\n",
      "16.12.2021  12:15    <DIR>          custom_simple\n",
      "16.12.2021  12:15            12˙888 data.py\n",
      "16.12.2021  12:15    <DIR>          dataset\n",
      "16.12.2021  12:39    <DIR>          deprecated\n",
      "16.12.2021  12:15             2˙734 dist_train.py\n",
      "16.12.2021  12:15             2˙040 generate.py\n",
      "16.12.2021  12:15             1˙079 LICENSE\n",
      "16.12.2021  12:15    <DIR>          midi\n",
      "16.12.2021  12:15    <DIR>          midi_classical\n",
      "16.12.2021  15:38    <DIR>          midi_processed\n",
      "16.12.2021  12:39    <DIR>          midi_processor\n",
      "16.12.2021  12:15            22˙151 model.py\n",
      "16.12.2021  12:15            22˙381 model_new.py\n",
      "16.12.2021  12:15            10˙536 music_transformer_train2.ipynb\n",
      "16.12.2021  16:20            52˙087 music-transformer-train.ipynb\n",
      "16.12.2021  12:15               409 params.py\n",
      "16.12.2021  15:38             4˙282 preprocess.py\n",
      "16.12.2021  12:15             4˙777 README.md\n",
      "16.12.2021  12:15    <DIR>          readme_src\n",
      "16.12.2021  15:44             2˙336 requirements.txt\n",
      "16.12.2021  12:15             1˙846 requirements_org.txt\n",
      "16.12.2021  12:15               603 simple_test_model.py\n",
      "16.12.2021  12:44               458 test.py\n",
      "16.12.2021  12:15             4˙603 train.py\n",
      "16.12.2021  12:15             6˙636 train_new.py\n",
      "16.12.2021  15:04    <DIR>          TRANSFORMER\n",
      "16.12.2021  12:15             7˙347 utils.py\n",
      "16.12.2021  12:38    <DIR>          venv\n",
      "              19 File(s)        159˙307 bytes\n",
      "              15 Dir(s)  67˙022˙348˙288 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zmvl1cFx_Nx_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490370332,
     "user_tz": -60,
     "elapsed": 545,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "81086941-3bb8-44a1-c465-7c18f6ff0908"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import wandb\n",
    "wandb.init()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "vj0pCvNqHPOL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490382816,
     "user_tz": -60,
     "elapsed": 9350,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "08ce9b62-3992-4f5e-9364-8042d6382ebb"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mnexon4444\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nexon4444/MusicTransformer/runs/uhcsx35p\" target=\"_blank\">charmed-lake-4</a></strong> to <a href=\"https://wandb.ai/nexon4444/MusicTransformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f282d33cd10>"
      ],
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/nexon4444/MusicTransformer/runs/uhcsx35p?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ],
   "metadata": {
    "id": "mOoIohRkIq0L",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490151507,
     "user_tz": -60,
     "elapsed": 1597,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [midi_classical\\alb_esp1_format0.mid]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"preprocess.py\", line 124, in <module>\n",
      "    save_dir=sys.argv[2])\n",
      "  File \"preprocess.py\", line 41, in preprocess_midi_files_under\n",
      "    with open('{}/{}.pickle'.format(save_dir,path.split('/')[-1]), 'wb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'midi_processed/midi_classical\\\\alb_esp1_format0.mid.pickle'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python preprocess.py midi_classical midi_processed\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VFpwQGh4_NyG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Nexon\\\\Documents\\\\Programowanie\\\\MusicTransformer'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "n1HXS45m_NyH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490385088,
     "user_tz": -60,
     "elapsed": 33,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "7bf64695-6fba-4fb4-a271-492bb48cfdc7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(os.listdir())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-SbQYeu_NyJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490386991,
     "user_tz": -60,
     "elapsed": 390,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "5856e29f-f822-4a0c-85c5-eb5fcc6784ce"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.idea', 'custom', 'custom_simple', 'data.py', 'dataset', 'deprecated', 'dist_train.py', 'generate.py', 'LICENSE', 'midi', 'midi_classical', 'midi_processed', 'midi_processor', 'model.py', 'model_new.py', 'music-transformer-train.ipynb', 'music_transformer_train2.ipynb', 'params.py', 'preprocess.py', 'README.md', 'readme_src', 'requirements.txt', 'requirements_org.txt', 'simple_test_model.py', 'test.py', 'train.py', 'train_new.py', 'TRANSFORMER', 'utils.py', 'venv', '__pycache__']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from model import MusicTransformer\n",
    "from custom.layers import *\n",
    "from custom import callback\n",
    "from tensorflow.python import keras\n",
    "# import params as par\n",
    "import midi_processor.processor as sequence\n",
    "from tensorflow.python import enable_eager_execution\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from data import Data\n",
    "import utils\n",
    "# tf.executing_eagerly()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "mEIEJHhE_NyJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490399368,
     "user_tz": -60,
     "elapsed": 2991,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdy4Nvwe_NyM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490411241,
     "user_tz": -60,
     "elapsed": 808,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "187a72de-05fd-44f8-e4e4-95a1995e9820"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "l_r = 0.001 #@param {type:\"slider\", min:0, max:0.1, step:0.0001}\n",
    "batch_size = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "pickle_dir = \"processed/\" #@param {type:\"string\"}\n",
    "max_seq = 2048 #@param {type:\"slider\", min:1, max:3000, step:1}\n",
    "epochs = 41 #@param {type:\"slider\", min:1, max:10000, step:1}\n",
    "model_save_path = \"bin/models\" #@param {type:\"string\"}\n",
    "embedding_dim = 256 #@param {type:\"slider\", min:2, max:2048, step:1}\n",
    "\n",
    "event_dim = sequence.RANGE_NOTE_ON + sequence.RANGE_NOTE_OFF + sequence.RANGE_TIME_SHIFT + sequence.RANGE_VEL\n",
    "vocab_size = event_dim + 3"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KqFoIEEz_NyO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490638692,
     "user_tz": -60,
     "elapsed": 251,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-3097facdc0f2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mmax_seq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_seq\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     debug=False)\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mmt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTransformerLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(identifier)\u001B[0m\n\u001B[1;32m    131\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m     raise ValueError(\n\u001B[0;32m--> 133\u001B[0;31m         'Could not interpret optimizer identifier: {}'.format(identifier))\n\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m: Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f27b063a290>"
     ]
    }
   ],
   "source": [
    "dataset = Data('dataset/processed/')\n",
    "opt = Adam(l_r)\n",
    "mt = MusicTransformer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layer=3,\n",
    "    max_seq=max_seq,\n",
    "    debug=False)\n",
    "mt.compile(optimizer=opt, loss=callback.TransformerLoss())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "SAMzVokw_NyQ",
    "executionInfo": {
     "status": "error",
     "timestamp": 1639490417507,
     "user_tz": -60,
     "elapsed": 2282,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "6c043ec0-d9db-48c8-fb72-652c7a0c3504"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "U5ZU-n3v_NyR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 17 11:35:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.06       Driver Version: 510.06       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 21%   49C    P8    10W / 180W |    765MiB /  6144MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1312    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1676    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      2384    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      2908    C+G   ...8\\jbr\\bin\\jcef_helper.exe    N/A      |\n",
      "|    0   N/A  N/A      3908    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4772    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      5124    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A      5644    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9000    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9696    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     10308    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11256    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11916    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     12200    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     12348    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     12484    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     12964    C+G   ...aming\\Spotify\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     13736    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     13748      C   ...r\\venv\\Scripts\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     13992    C+G   ...3.0.8.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     16448    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     18648    C+G   ...fyw5nnt\\app\\Messenger.exe    N/A      |\n",
      "|    0   N/A  N/A     19344    C+G   ...\\app-1.0.9003\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     21540    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zUC43qq_NyT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490717933,
     "user_tz": -60,
     "elapsed": 369,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "ec689d5f-e694-4799-f4a3-452d937f998d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: $[[367  64 370 ... 349 204 221]]\n",
      "2\n",
      "x: $[[ 33 308 369 ...  71 366  65]]\n",
      "2\n",
      "x: $[[200 274 202 ... 264 203 366]]\n",
      "2\n",
      "x: $[[ 61 369  69 ...  81 287 201]]\n",
      "x: $[[373  76 284 ... 271 371  67]]\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"music_transformer_3\" (type MusicTransformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\model_new.py\", line 52, in call  *\n            encoder, weight_encoder = self.Encoder(inputs, training=training, mask=src_mask)\n        File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer \"encoder_3\" (type Encoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\custom\\layers.py\", line 376, in call  *\n                x, w = self.enc_layers[i](x, mask, training=training)\n            File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            TypeError: Exception encountered when calling layer \"encoder_layer_18\" (type EncoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\custom\\layers.py\", line 292, in call  *\n                    attn_out, w = self.rga([x,x,x], mask)\n                File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                TypeError: Exception encountered when calling layer \"relative_global_attention_54\" (type RelativeGlobalAttention).\n                \n                in user code:\n                \n                    File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\custom\\layers.py\", line 195, in call  *\n                        q = tf.reshape(q, (q.shape[0], q.shape[1], self.h, -1))\n                \n                    TypeError: Failed to convert elements of (None, 2048, 4, -1) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                \n                \n                Call arguments received:\n                  • inputs=['tf.Tensor(shape=(None, 2048, 256), dtype=float32)', 'tf.Tensor(shape=(None, 2048, 256), dtype=float32)', 'tf.Tensor(shape=(None, 2048, 256), dtype=float32)']\n                  • mask=None\n                  • kwargs={'training': 'True'}\n            \n            \n            Call arguments received:\n              • x=tf.Tensor(shape=(None, 2048, 256), dtype=float32)\n              • mask=None\n              • training=True\n              • kwargs=<class 'inspect._empty'>\n        \n        \n        Call arguments received:\n          • x=tf.Tensor(shape=(None, 2048), dtype=int32)\n          • mask=None\n          • training=True\n    \n    \n    Call arguments received:\n      • inputs=('tf.Tensor(shape=(None, 2048), dtype=int32)', 'tf.Tensor(shape=(None, 2048), dtype=int32)')\n      • training=True\n      • eval=None\n      • src_mask=None\n      • trg_mask=None\n      • lookup_mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_13748/3127677715.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerators_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[1;31m# ds = tf.data.Dataset.from_generator(dataset.generators_dict[\"train\"].__iter__(), output_types=tf.float32)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m \u001B[0mmt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerators_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerators_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    154\u001B[0m \u001B[1;31m# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[1;31m# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mautograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1127\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1128\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ag_error_metadata\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1129\u001B[1;33m               \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1130\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1131\u001B[0m               \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"music_transformer_3\" (type MusicTransformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\model_new.py\", line 52, in call  *\n            encoder, weight_encoder = self.Encoder(inputs, training=training, mask=src_mask)\n        File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer \"encoder_3\" (type Encoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\custom\\layers.py\", line 376, in call  *\n                x, w = self.enc_layers[i](x, mask, training=training)\n            File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            TypeError: Exception encountered when calling layer \"encoder_layer_18\" (type EncoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\custom\\layers.py\", line 292, in call  *\n                    attn_out, w = self.rga([x,x,x], mask)\n                File \"c:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                TypeError: Exception encountered when calling layer \"relative_global_attention_54\" (type RelativeGlobalAttention).\n                \n                in user code:\n                \n                    File \"C:\\Users\\Nexon\\Documents\\Programowanie\\MusicTransformer\\custom\\layers.py\", line 195, in call  *\n                        q = tf.reshape(q, (q.shape[0], q.shape[1], self.h, -1))\n                \n                    TypeError: Failed to convert elements of (None, 2048, 4, -1) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                \n                \n                Call arguments received:\n                  • inputs=['tf.Tensor(shape=(None, 2048, 256), dtype=float32)', 'tf.Tensor(shape=(None, 2048, 256), dtype=float32)', 'tf.Tensor(shape=(None, 2048, 256), dtype=float32)']\n                  • mask=None\n                  • kwargs={'training': 'True'}\n            \n            \n            Call arguments received:\n              • x=tf.Tensor(shape=(None, 2048, 256), dtype=float32)\n              • mask=None\n              • training=True\n              • kwargs=<class 'inspect._empty'>\n        \n        \n        Call arguments received:\n          • x=tf.Tensor(shape=(None, 2048), dtype=int32)\n          • mask=None\n          • training=True\n    \n    \n    Call arguments received:\n      • inputs=('tf.Tensor(shape=(None, 2048), dtype=int32)', 'tf.Tensor(shape=(None, 2048), dtype=int32)')\n      • training=True\n      • eval=None\n      • src_mask=None\n      • trg_mask=None\n      • lookup_mask=None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from model_test import MusicTransformer\n",
    "from custom.layers import *\n",
    "from custom import callback\n",
    "import params as par\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from data import DataNew\n",
    "import utils\n",
    "import argparse\n",
    "import datetime\n",
    "import midi_processor.processor as sequence\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IS_ON_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IS_ON_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "\n",
    "tf.executing_eagerly()\n",
    "\n",
    "num_layer = 6\n",
    "l_r = 0.001 #@param {type:\"slider\", min:0, max:0.1, step:0.0001}\n",
    "batch_size = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "pickle_dir = \"processed/\" #@param {type:\"string\"}\n",
    "max_seq = 2048 #@param {type:\"slider\", min:1, max:3000, step:1}\n",
    "epochs = 41 #@param {type:\"slider\", min:1, max:10000, step:1}\n",
    "model_save_path = \"bin/models\" #@param {type:\"string\"}\n",
    "embedding_dim = 256 #@param {type:\"slider\", min:2, max:2048, step:1}\n",
    "\n",
    "event_dim = sequence.RANGE_NOTE_ON + sequence.RANGE_NOTE_OFF + sequence.RANGE_TIME_SHIFT + sequence.RANGE_VEL\n",
    "vocab_size = event_dim + 3\n",
    "\n",
    "\n",
    "def get_current_datetime():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    dt_name = now.strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "    return dt_name\n",
    "\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    FOLDER_ROOT = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2\"\n",
    "else:\n",
    "    FOLDER_ROOT = \".\"\n",
    "\n",
    "TEST_RUN = True\n",
    "NORMALIZE_NOTES = True\n",
    "USE_COMPUTED_VALUES = True\n",
    "USE_SAVE_POINT = False\n",
    "\n",
    "NORMALIZATION_BOUNDARIES = [3, 4]\n",
    "EPOCHS = 250\n",
    "LATENT_VECTOR_DIM = 2\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 32\n",
    "\n",
    "FOLDER_ROOT = \".\"\n",
    "# COMPUTED_INT_TO_NOTE_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/dicts/int_to_note_08_19_2021__17_25_44\"\n",
    "# COMPUTED_INT_TO_DURATION_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/dicts/int_to_duration_08_19_2021__17_25_44\"\n",
    "# COMPUTED_NOTES_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/notes/notes_08_19_2021__17_25_44\"\n",
    "# COMPUTED_DURATIONS_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/durations/durations_08_19_2021__17_25_44\"\n",
    "COMPUTED_DATA_PATH = \"AUTOENCODER/data/data_file_12_06_2021__19_53_42\"\n",
    "\n",
    "SAVE_POINT = \"AUTOENCODER/checkpoints/08_19_2021__18_34_10/epoch=014-loss=383.5284-acc=0.0000.hdf5\"\n",
    "AUTOENCODER = \"TRANSFORMER\"\n",
    "\n",
    "MODEL_NAME = AUTOENCODER\n",
    "MODEL_FOLDER_ROOT = os.path.join(FOLDER_ROOT, MODEL_NAME)\n",
    "CURR_DT = get_current_datetime()\n",
    "MODEL_DIR_PATH = os.path.join(MODEL_FOLDER_ROOT, \"generated_models\")\n",
    "OCCURENCES = os.path.join(MODEL_FOLDER_ROOT, \"data\", \"occurences\")\n",
    "\n",
    "DATA_DIR = os.path.join(MODEL_FOLDER_ROOT, \"data\")\n",
    "DATA_NOTES_DIR = os.path.join(DATA_DIR, \"notes\")\n",
    "DATA_DURATIONS_DIR = os.path.join(DATA_DIR, \"durations\")\n",
    "\n",
    "DATA_FILE_PATH = os.path.join(DATA_DIR, \"data_file_\" + str(CURR_DT))\n",
    "\n",
    "DATA_DICTS_DIR = os.path.join(DATA_DIR, \"dicts\")\n",
    "DATA_INT_TO_NOTE_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_note_\" + str(CURR_DT))\n",
    "DATA_INT_TO_DURATION_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_duration_\" + str(CURR_DT))\n",
    "DATA_NOTES_PATH = os.path.join(DATA_NOTES_DIR, \"notes_\" + str(CURR_DT))\n",
    "\n",
    "DATA_DURATIONS_PATH = os.path.join(DATA_DURATIONS_DIR, \"durations_\" + str(CURR_DT))\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs\")\n",
    "MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_smaller\")\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_medium\")\n",
    "MIDI_GENERATED_DIR = os.path.join(MODEL_FOLDER_ROOT, \"midi_generated\")\n",
    "MIDI_SONGS_REGEX = os.path.join(MIDI_SONGS_DIR, \"*.mid\")\n",
    "CHECKPOINTS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"checkpoints\")\n",
    "CHECKPOINT = os.path.join(CHECKPOINTS_DIR, str(CURR_DT))\n",
    "LOGS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"logs\")\n",
    "\n",
    "LOG = os.path.join(LOGS_DIR, str(CURR_DT))\n",
    "all_paths = [MODEL_DIR_PATH, OCCURENCES, DATA_NOTES_DIR, DATA_DURATIONS_DIR, DATA_DICTS_DIR,\n",
    "             MIDI_GENERATED_DIR, CHECKPOINTS_DIR, CHECKPOINT, LOGS_DIR, LOG]\n",
    "\n",
    "for path in all_paths:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "# load data\n",
    "dataset = DataNew('midi_processed', max_seq, batch_size)\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "# load model\n",
    "curr_dt = get_current_datetime()\n",
    "learning_rate = callback.CustomSchedule(par.embedding_dim) if l_r is None else l_r\n",
    "opt = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "filepath = CHECKPOINT + str(curr_dt) + \"/\" + \"epoch:{epoch:02d}-loss:{loss:.4f}\" #-acc:{binary_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "#\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     filepath,\n",
    "#     monitor='binary_accuracy',\n",
    "#     verbose=0,\n",
    "#     save_best_only=True,\n",
    "#     mode='max'\n",
    "# )\n",
    "log = tf.keras.callbacks.TensorBoard(log_dir=LOG + curr_dt),\n",
    "\n",
    "# callbacks_list = [checkpoint, log]\n",
    "callbacks_list = [log]\n",
    "# define model\n",
    "mt = MusicTransformer(\n",
    "            embedding_dim=256,\n",
    "            vocab_size=par.vocab_size,\n",
    "            num_layer=num_layer,\n",
    "            max_seq=max_seq,\n",
    "            dropout=0.2,\n",
    "            debug=False, loader_path=None)\n",
    "mt.compile(optimizer=opt, loss=callback.transformer_dist_train_loss)\n",
    "# mt.run_eagerly = True\n",
    "# batch = (dataset.generators_dict[\"train\"][0])\n",
    "# print(type(dataset.generators_dict[\"train\"]))\n",
    "# mt.train_on_batch(batch[0], batch[1])\n",
    "print(len(dataset.generators_dict[\"train\"][0]))\n",
    "print(len(dataset.generators_dict[\"train\"][1]))\n",
    "print(len(dataset.generators_dict[\"train\"][2]))\n",
    "# ds = tf.data.Dataset.from_generator(dataset.generators_dict[\"train\"].__iter__(), output_types=tf.float32)\n",
    "mt.fit(x=dataset.generators_dict[\"train\"][0], y=dataset.generators_dict[\"train\"][1], epochs=EPOCHS, callbacks=callbacks_list)\n",
    "# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\n",
    "# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\n",
    "\n",
    "# define tensorboard writer\n",
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "train_log_dir = 'logs/mt_decoder/'+current_time+'/train'\n",
    "eval_log_dir = 'logs/mt_decoder/'+current_time+'/eval'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "eval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\n",
    "EPOCHS=2\n",
    "\n",
    "# Train Start\n",
    "# history = mt.fit(dataset.slide_seq2seq_batch(batch_size, max_seq), epochs=EPOCHS)\n",
    "\n",
    "# idx = 0\n",
    "# for e in range(epochs):\n",
    "#     mt.reset_metrics()\n",
    "#     for b in range(len(dataset.files) // batch_size):\n",
    "#         try:\n",
    "#             batch_x, batch_y = dataset.slide_seq2seq_batch(batch_size, max_seq)\n",
    "#         except:\n",
    "#             continue\n",
    "#         result_metrics = mt.train_on_batch(batch_x, batch_y)\n",
    "#         if b % 100 == 0:\n",
    "#             eval_x, eval_y = dataset.slide_seq2seq_batch(batch_size, max_seq, 'eval')\n",
    "#             eval_result_metrics, weights = mt.evaluate(eval_x, eval_y)\n",
    "#             mt.save(save_path)\n",
    "#             with train_summary_writer.as_default():\n",
    "#                 if b == 0:\n",
    "#                     tf.summary.histogram(\"target_analysis\", batch_y, step=e)\n",
    "#                     tf.summary.histogram(\"source_analysis\", batch_x, step=e)\n",
    "#\n",
    "#                 tf.summary.scalar('loss', result_metrics[0], step=idx)\n",
    "#                 tf.summary.scalar('accuracy', result_metrics[1], step=idx)\n",
    "#\n",
    "#             with eval_summary_writer.as_default():\n",
    "#                 if b == 0:\n",
    "#                     mt.sanity_check(eval_x, eval_y, step=e)\n",
    "#\n",
    "#                 tf.summary.scalar('loss', eval_result_metrics[0], step=idx)\n",
    "#                 tf.summary.scalar('accuracy', eval_result_metrics[1], step=idx)\n",
    "#                 for i, weight in enumerate(weights):\n",
    "#                     with tf.name_scope(\"layer_%d\" % i):\n",
    "#                         with tf.name_scope(\"w\"):\n",
    "#                             utils.attention_image_summary(weight, step=idx)\n",
    "#                 # for i, weight in enumerate(weights):\n",
    "#                 #     with tf.name_scope(\"layer_%d\" % i):\n",
    "#                 #         with tf.name_scope(\"_w0\"):\n",
    "#                 #             utils.attention_image_summary(weight[0])\n",
    "#                 #         with tf.name_scope(\"_w1\"):\n",
    "#                 #             utils.attention_image_summary(weight[1])\n",
    "#             idx += 1\n",
    "#             print('\\n====================================================')\n",
    "#             print('Epoch/Batch: {}/{}'.format(e, b))\n",
    "#             print('Train >>>> Loss: {:6.6}, Accuracy: {}'.format(result_metrics[0], result_metrics[1]))\n",
    "#             print('Eval >>>> Loss: {:6.6}, Accuracy: {}'.format(eval_result_metrics[0], eval_result_metrics[1]))\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yf7X6VV8_NyU",
    "executionInfo": {
     "status": "error",
     "timestamp": 1639490690233,
     "user_tz": -60,
     "elapsed": 47581,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "1ac1e2e0-bb97-4b1e-8956-d1eb95730605"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for e in range(epochs):\n",
    "#     for b in range(len(dataset.files) // batch_size):\n",
    "#         batch_x, batch_y = dataset.seq2seq_batch(batch_size, par.max_seq)\n",
    "#         result_metrics = mt.train_on_batch(batch_x, batch_y)\n",
    "#         if b % 1 == 0:\n",
    "#           print('===========================================\\n')\n",
    "#           print('Loss: {:6.6}, Accuracy: {:3.2}'.format(result_metrics[0], result_metrics[1]))\n",
    "#           mt.save('mt-2048-h4-dim256.h5', )\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GIsCBH0e_NyW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MusicTransformer"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QR77-ehl_NyX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python train.py --epochs=1 --pickle_dir=\"dataset/processed\" --save_path=\"bin/models\" --max_seq=2048  --batch_size=1 --l_r=0.001"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IVOZPogN_NyZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "UQp14qG1_NyZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fill_with_placeholder(prev_data: list, max_len: int, max_val: int=239):\n",
    "    placeholder = [max_val for _ in range(max_len - len(prev_data))]\n",
    "    return placeholder+prev_data"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ERaK3zJq_Nya"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(max_seq):\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "j1dFNr9c_Nyb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:Assets written to: xxx\\assets\n",
      "106/106 - 4s - loss: 2.7052 - accuracy: 0.6200 - val_loss: 0.6497 - val_accuracy: 0.8102 - 4s/epoch - 37ms/step\n",
      "Epoch 2/20\n",
      "INFO:tensorflow:Assets written to: xxx\\assets\n",
      "106/106 - 1s - loss: 0.5000 - accuracy: 0.8535 - val_loss: 0.3908 - val_accuracy: 0.8858 - 1s/epoch - 13ms/step\n",
      "Epoch 3/20\n",
      "INFO:tensorflow:Assets written to: xxx\\assets\n",
      "106/106 - 1s - loss: 0.3328 - accuracy: 0.9023 - val_loss: 0.3134 - val_accuracy: 0.9092 - 1s/epoch - 13ms/step\n",
      "Epoch 4/20\n",
      "INFO:tensorflow:Assets written to: xxx\\assets\n",
      "106/106 - 1s - loss: 0.2601 - accuracy: 0.9238 - val_loss: 0.2788 - val_accuracy: 0.9202 - 1s/epoch - 14ms/step\n",
      "Epoch 5/20\n",
      "INFO:tensorflow:Assets written to: xxx\\assets\n",
      "106/106 - 1s - loss: 0.2178 - accuracy: 0.9350 - val_loss: 0.2476 - val_accuracy: 0.9282 - 1s/epoch - 14ms/step\n",
      "Epoch 6/20\n",
      "INFO:tensorflow:Assets written to: xxx\\assets\n",
      "106/106 - 1s - loss: 0.1858 - accuracy: 0.9438 - val_loss: 0.2389 - val_accuracy: 0.9338 - 1s/epoch - 14ms/step\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_13748/2608808408.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[1;31m# training the NN\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[0mepochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m512\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_validate\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_validate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Finished fitting.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1214\u001B[0m                 _r=1):\n\u001B[0;32m   1215\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1216\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1217\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    909\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 910\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    911\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    912\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    940\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    941\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 942\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    943\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    944\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3129\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m   3130\u001B[0m     return graph_function._call_flat(\n\u001B[1;32m-> 3131\u001B[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m   3132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3133\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1958\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1959\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1960\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1961\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1962\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    601\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    602\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 603\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    604\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    605\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mc:\\users\\nexon\\documents\\programowanie\\musictransformer\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 59\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     60\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# suppress warning/error messages in terminal\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# load in data from mnist dataset (60k training, 10k test)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape and convert to one-hot\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# split main training set into train/validation sets (6k out of 60k data points reserved for validation)\n",
    "x_validate = x_train[:6000, :]\n",
    "y_validate = y_train[:6000, :]\n",
    "x_train = x_train[6000:, :]\n",
    "y_train = y_train[6000:, :]\n",
    "\n",
    "'''\n",
    "Building layers for the feedforward NN:\n",
    "Hidden layers have 56, 96, 96, and 56 nodes, in that order.\n",
    "Takes in 784 values (pixel input) and outputs 10 values (predicted probability for each number, 0-9).\n",
    "'''\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(56,activation='relu',input_shape=(784,)))\n",
    "model.add(layers.Dense(96,activation='relu'))\n",
    "model.add(layers.Dense(96,activation='relu'))\n",
    "model.add(layers.Dense(56,activation='relu'))\n",
    "model.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "filepath = \"xxx\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "#\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     filepath,\n",
    "#     monitor='binary_accuracy',\n",
    "#     verbose=0,\n",
    "#     save_best_only=True,\n",
    "#     mode='max'\n",
    "# )\n",
    "log = tf.keras.callbacks.TensorBoard(log_dir=LOG + curr_dt),\n",
    "\n",
    "callbacks_list = [checkpoint, log]\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "\t\t\t  optimizer='adam',\n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "# training the NN\n",
    "epochs = 20\n",
    "history = model.fit(x_train,y_train,epochs=epochs,batch_size=512,verbose=2,validation_data=(x_validate,y_validate), callbacks=callbacks_list)\n",
    "print(\"Finished fitting.\")\n",
    "\n",
    "# plotting learning curves during training (on both training and validation data)\n",
    "epoch_labels = range(1, epochs+1)\n",
    "hist_dict = history.history\n",
    "# plt.title(\"Accuracy vs Epochs\")\n",
    "# plt.plot(epoch_labels, hist_dict[\"acc\"],'bo', label=\"Training\")\n",
    "# plt.plot(epoch_labels, hist_dict[\"val_acc\"],'go', label=\"Validation\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# evaluating final performance using test data\n",
    "print(\"Checking accuracy on test set...\")\n",
    "acc = model.evaluate(x_test,y_test,batch_size=512)\n",
    "print(\"\\nAccuracy on test set: \" + str(acc[1]))\n",
    "# plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IkS0hRyn_Nyb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "id": "KjwIFHNWPG5g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490622979,
     "user_tz": -60,
     "elapsed": 485,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   },
   "execution_count": 23,
   "outputs": []
  }
 ]
}