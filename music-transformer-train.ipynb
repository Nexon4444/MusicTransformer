{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "music-transformer-train.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install progress\n",
    "!pip install pretty_midi\n",
    "!pip install wandb"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kDWpEd95_NyB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Sll3ovb_Nx4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490342022,
     "user_tz": -60,
     "elapsed": 2658,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "4a7b5c4b-89eb-4021-e77e-d69003b07ff5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/My Drive/MusicTransformer\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/'My Drive'/MusicTransformer\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hq70x3Ds_Nx8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490367295,
     "user_tz": -60,
     "elapsed": 256,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "4ac20099-382a-4094-fcc3-6f59784aac39"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[0m\u001B[01;34mAUTOENCODER\u001B[0m/     \u001B[01;34mmidi_classical\u001B[0m/                 README.md\n",
      "\u001B[01;34mbin\u001B[0m/             \u001B[01;34mmidi_prepared_small\u001B[0m/            \u001B[01;34mreadme_src\u001B[0m/\n",
      "\u001B[01;34mcustom\u001B[0m/          \u001B[01;34mmidi_processed\u001B[0m/                 requirements_org.txt\n",
      "\u001B[01;34mcustom_simple\u001B[0m/   \u001B[01;34mmidi_processed_small\u001B[0m/           requirements.txt\n",
      "data.py          \u001B[01;34mmidi_processor\u001B[0m/                 simple_test_model.py\n",
      "\u001B[01;34mdataset\u001B[0m/         model_new.py                    test.py\n",
      "\u001B[01;34mdeprecated\u001B[0m/      model.py                        train_new.py\n",
      "dist_train.py    \u001B[01;34mmt-2048-h4-dim256.h5\u001B[0m/           train.py\n",
      "generate.py      music_transformer_train2.ipynb  \u001B[01;34mTRANSFORMER\u001B[0m/\n",
      "\u001B[01;34mhidi_processed\u001B[0m/  music-transformer-train.ipynb   utils.py\n",
      "LICENSE          params.py                       \u001B[01;34mwandb\u001B[0m/\n",
      "\u001B[01;34mlogs\u001B[0m/            preprocess.py\n",
      "\u001B[01;34mmidi\u001B[0m/            \u001B[01;34m__pycache__\u001B[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zmvl1cFx_Nx_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490370332,
     "user_tz": -60,
     "elapsed": 545,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "81086941-3bb8-44a1-c465-7c18f6ff0908"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import wandb\n",
    "wandb.init()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "vj0pCvNqHPOL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490382816,
     "user_tz": -60,
     "elapsed": 9350,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "08ce9b62-3992-4f5e-9364-8042d6382ebb"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mnexon4444\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nexon4444/MusicTransformer/runs/uhcsx35p\" target=\"_blank\">charmed-lake-4</a></strong> to <a href=\"https://wandb.ai/nexon4444/MusicTransformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f282d33cd10>"
      ],
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/nexon4444/MusicTransformer/runs/uhcsx35p?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from numba import cuda \n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()"
   ],
   "metadata": {
    "id": "mOoIohRkIq0L",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490151507,
     "user_tz": -60,
     "elapsed": 1597,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "!python preprocess.py midi_classical midi_processed\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VFpwQGh4_NyG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/MusicTransformer'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "n1HXS45m_NyH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490385088,
     "user_tz": -60,
     "elapsed": 33,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "7bf64695-6fba-4fb4-a271-492bb48cfdc7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(os.listdir())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-SbQYeu_NyJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490386991,
     "user_tz": -60,
     "elapsed": 390,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "5856e29f-f822-4a0c-85c5-eb5fcc6784ce"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['custom', 'dataset', 'logs', '__pycache__', '.idea', 'midi_processed', 'midi', 'readme_src', 'deprecated', 'midi_classical', 'midi_processor', '.git', 'hidi_processed', '.gitignore', 'LICENSE', 'README.md', 'dist_train.py', 'generate.py', 'preprocess.py', 'utils.py', 'requirements_org.txt', 'requirements.txt', 'midi_prepared_small', '.ipynb_checkpoints', 'mt-2048-h4-dim256.h5', 'params.py', 'midi_processed_small', 'custom_simple', 'bin', 'TRANSFORMER', 'AUTOENCODER', 'model_new.py', 'music_transformer_train2.ipynb', 'simple_test_model.py', 'test.py', 'train_new.py', '.gdriveignore', 'data.py', 'model.py', 'music-transformer-train.ipynb', 'train.py', 'wandb']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from model import MusicTransformer\n",
    "from custom.layers import *\n",
    "from custom import callback\n",
    "from tensorflow.python import keras\n",
    "# import params as par\n",
    "import midi_processor.processor as sequence\n",
    "from tensorflow.python import enable_eager_execution\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from data import Data\n",
    "import utils\n",
    "# tf.executing_eagerly()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "mEIEJHhE_NyJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490399368,
     "user_tz": -60,
     "elapsed": 2991,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-2c6538b30a46>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdy4Nvwe_NyM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490411241,
     "user_tz": -60,
     "elapsed": 808,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "187a72de-05fd-44f8-e4e4-95a1995e9820"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "l_r = 0.001 #@param {type:\"slider\", min:0, max:0.1, step:0.0001}\n",
    "batch_size = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "pickle_dir = \"processed/\" #@param {type:\"string\"}\n",
    "max_seq = 2048 #@param {type:\"slider\", min:1, max:3000, step:1}\n",
    "epochs = 41 #@param {type:\"slider\", min:1, max:10000, step:1}\n",
    "model_save_path = \"bin/models\" #@param {type:\"string\"}\n",
    "embedding_dim = 256 #@param {type:\"slider\", min:2, max:2048, step:1}\n",
    "\n",
    "event_dim = sequence.RANGE_NOTE_ON + sequence.RANGE_NOTE_OFF + sequence.RANGE_TIME_SHIFT + sequence.RANGE_VEL\n",
    "vocab_size = event_dim + 3"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KqFoIEEz_NyO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490638692,
     "user_tz": -60,
     "elapsed": 251,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-3097facdc0f2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mmax_seq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_seq\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     debug=False)\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mmt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTransformerLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(identifier)\u001B[0m\n\u001B[1;32m    131\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m     raise ValueError(\n\u001B[0;32m--> 133\u001B[0;31m         'Could not interpret optimizer identifier: {}'.format(identifier))\n\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m: Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f27b063a290>"
     ]
    }
   ],
   "source": [
    "dataset = Data('dataset/processed/')\n",
    "opt = Adam(l_r)\n",
    "mt = MusicTransformer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layer=3,\n",
    "    max_seq=max_seq,\n",
    "    debug=False)\n",
    "mt.compile(optimizer=opt, loss=callback.TransformerLoss())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "SAMzVokw_NyQ",
    "executionInfo": {
     "status": "error",
     "timestamp": 1639490417507,
     "user_tz": -60,
     "elapsed": 2282,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "6c043ec0-d9db-48c8-fb72-652c7a0c3504"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "U5ZU-n3v_NyR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tue Dec 14 14:05:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   64C    P0    61W / 149W |  10990MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zUC43qq_NyT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490717933,
     "user_tz": -60,
     "elapsed": 369,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "ec689d5f-e694-4799-f4a3-452d937f998d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: $[[366  65 366 ...  77 367  65]]\n",
      "2\n",
      "x: $[[370  60 266 ... 371  72 371]]\n",
      "2\n",
      "x: $[[370  62 374 ... 186 210 367]]\n",
      "2\n",
      "x: $[[230 218 225 ... 369  70 369]]\n",
      "x: $[[366  60 283 ... 373  77 264]]\n",
      "Epoch 1/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 3s 3s/step - loss: 6.5308 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0548 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7730 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5701 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.4233 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 6/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3054 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2098 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 8/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1288 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 9/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0609 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 10/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0025 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 11/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9517 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 12/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9076 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 13/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8688 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 14/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8363 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 15/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8066 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 16/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7797 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 17/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7559 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 18/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7344 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 19/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7151 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 20/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6974 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 21/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6814 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 22/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6665 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 23/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6528 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 24/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6398 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 25/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6283 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 26/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6176 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 27/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6075 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 28/250\n",
      "targets: [[218 225 295 ...  70 369  66]]\n",
      "inputs: [[230 218 225 ... 369  70 369]]\n",
      "type(x): <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "x: [[230 218 225 ... 369  70 369]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n",
      "out2.shape: (1, 2048, 256)\n",
      "w.shape: (1, 4, 2048, 2048)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-d3b8af699742>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerators_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"train\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[0;31m# ds = tf.data.Dataset.from_generator(dataset.generators_dict[\"train\"].__iter__(), output_types=tf.float32)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m \u001B[0mmt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerators_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"train\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerators_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"train\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m \u001B[0;31m# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[0;31m# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/My Drive/MusicTransformer/model_new.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs, training, eval, src_mask, trg_mask, lookup_mask)\u001B[0m\n\u001B[1;32m     51\u001B[0m         \u001B[0mencoder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight_encoder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEncoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msrc_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m         decoder, weights = self.Decoder(\n\u001B[0;32m---> 53\u001B[0;31m             \u001B[0mtargets\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menc_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlookup_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlookup_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrg_mask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m         )\n\u001B[1;32m     55\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/My Drive/MusicTransformer/custom/layers.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, x, mask, lookup_mask, training, enc_output)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    404\u001B[0m             \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw2\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 405\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdec_layers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menc_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlookup_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlookup_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw_out\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    406\u001B[0m             \u001B[0mweights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    407\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/My Drive/MusicTransformer/custom/layers.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, x, encode_out, mask, lookup_mask, training, w_out, **kwargs)\u001B[0m\n\u001B[1;32m    330\u001B[0m             \u001B[0mattn_out2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maw2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrga2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mout1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m             \u001B[0mattn_out2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maw2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrga2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mout1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencode_out\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencode_out\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m         \u001B[0mattn_out2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_out2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m         \u001B[0mattn_out2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayernorm2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout1\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mattn_out2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/My Drive/MusicTransformer/custom/layers.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs, mask, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m             \u001B[0mlogits\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1e9\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 225\u001B[0;31m         \u001B[0mattention_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    226\u001B[0m         \u001B[0;31m# tf.print('logit result: \\n', logits, output_stream=sys.stdout)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mResourceExhaustedError\u001B[0m: Exception encountered when calling layer \"relative_global_attention_16\" (type RelativeGlobalAttention).\n\nOOM when allocating tensor with shape[1,4,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]\n\nCall arguments received:\n  • inputs=['tf.Tensor(shape=(1, 2048, 256), dtype=float32)', 'tf.Tensor(shape=(1, 2048, 256), dtype=float32)', 'tf.Tensor(shape=(1, 2048, 256), dtype=float32)']\n  • mask=None\n  • kwargs={'training': 'True'}"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from model_new import MusicTransformer\n",
    "from custom.layers import *\n",
    "from custom import callback\n",
    "import params as par\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from data import DataNew\n",
    "import utils\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IS_ON_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IS_ON_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "\n",
    "# tf.executing_eagerly()\n",
    "\n",
    "num_layer = 6\n",
    "\n",
    "\n",
    "def get_current_datetime():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    dt_name = now.strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "    return dt_name\n",
    "\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    FOLDER_ROOT = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2\"\n",
    "else:\n",
    "    FOLDER_ROOT = \".\"\n",
    "\n",
    "TEST_RUN = True\n",
    "NORMALIZE_NOTES = True\n",
    "USE_COMPUTED_VALUES = True\n",
    "USE_SAVE_POINT = False\n",
    "\n",
    "NORMALIZATION_BOUNDARIES = [3, 4]\n",
    "EPOCHS = 250\n",
    "LATENT_VECTOR_DIM = 2\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 32\n",
    "\n",
    "FOLDER_ROOT = \".\"\n",
    "# COMPUTED_INT_TO_NOTE_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/dicts/int_to_note_08_19_2021__17_25_44\"\n",
    "# COMPUTED_INT_TO_DURATION_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/dicts/int_to_duration_08_19_2021__17_25_44\"\n",
    "# COMPUTED_NOTES_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/notes/notes_08_19_2021__17_25_44\"\n",
    "# COMPUTED_DURATIONS_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/durations/durations_08_19_2021__17_25_44\"\n",
    "COMPUTED_DATA_PATH = \"AUTOENCODER/data/data_file_12_06_2021__19_53_42\"\n",
    "\n",
    "SAVE_POINT = \"AUTOENCODER/checkpoints/08_19_2021__18_34_10/epoch=014-loss=383.5284-acc=0.0000.hdf5\"\n",
    "AUTOENCODER = \"TRANSFORMER\"\n",
    "\n",
    "MODEL_NAME = AUTOENCODER\n",
    "MODEL_FOLDER_ROOT = os.path.join(FOLDER_ROOT, MODEL_NAME)\n",
    "CURR_DT = get_current_datetime()\n",
    "MODEL_DIR_PATH = os.path.join(MODEL_FOLDER_ROOT, \"generated_models\")\n",
    "OCCURENCES = os.path.join(MODEL_FOLDER_ROOT, \"data\", \"occurences\")\n",
    "\n",
    "DATA_DIR = os.path.join(MODEL_FOLDER_ROOT, \"data\")\n",
    "DATA_NOTES_DIR = os.path.join(DATA_DIR, \"notes\")\n",
    "DATA_DURATIONS_DIR = os.path.join(DATA_DIR, \"durations\")\n",
    "\n",
    "DATA_FILE_PATH = os.path.join(DATA_DIR, \"data_file_\" + str(CURR_DT))\n",
    "\n",
    "DATA_DICTS_DIR = os.path.join(DATA_DIR, \"dicts\")\n",
    "DATA_INT_TO_NOTE_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_note_\" + str(CURR_DT))\n",
    "DATA_INT_TO_DURATION_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_duration_\" + str(CURR_DT))\n",
    "DATA_NOTES_PATH = os.path.join(DATA_NOTES_DIR, \"notes_\" + str(CURR_DT))\n",
    "\n",
    "DATA_DURATIONS_PATH = os.path.join(DATA_DURATIONS_DIR, \"durations_\" + str(CURR_DT))\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs\")\n",
    "MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_smaller\")\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_medium\")\n",
    "MIDI_GENERATED_DIR = os.path.join(MODEL_FOLDER_ROOT, \"midi_generated\")\n",
    "MIDI_SONGS_REGEX = os.path.join(MIDI_SONGS_DIR, \"*.mid\")\n",
    "CHECKPOINTS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"checkpoints\")\n",
    "CHECKPOINT = os.path.join(CHECKPOINTS_DIR, str(CURR_DT))\n",
    "LOGS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"logs\")\n",
    "\n",
    "LOG = os.path.join(LOGS_DIR, str(CURR_DT))\n",
    "all_paths = [MODEL_DIR_PATH, OCCURENCES, DATA_NOTES_DIR, DATA_DURATIONS_DIR, DATA_DICTS_DIR,\n",
    "             MIDI_GENERATED_DIR, CHECKPOINTS_DIR, CHECKPOINT, LOGS_DIR, LOG]\n",
    "\n",
    "for path in all_paths:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "# load data\n",
    "dataset = DataNew('midi_processed', max_seq, batch_size)\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "# load model\n",
    "curr_dt = get_current_datetime()\n",
    "learning_rate = callback.CustomSchedule(par.embedding_dim) if l_r is None else l_r\n",
    "opt = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "filepath = CHECKPOINT + str(curr_dt) + \"/\" + \"epoch:{epoch:02d}-loss:{loss:.4f}\" #-acc:{binary_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "#\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     filepath,\n",
    "#     monitor='binary_accuracy',\n",
    "#     verbose=0,\n",
    "#     save_best_only=True,\n",
    "#     mode='max'\n",
    "# )\n",
    "log = tf.keras.callbacks.TensorBoard(log_dir=LOG + curr_dt),\n",
    "\n",
    "# callbacks_list = [checkpoint, log]\n",
    "callbacks_list = [log]\n",
    "# define model\n",
    "mt = MusicTransformer(\n",
    "            embedding_dim=256,\n",
    "            vocab_size=par.vocab_size,\n",
    "            num_layer=num_layer,\n",
    "            max_seq=max_seq,\n",
    "            dropout=0.2,\n",
    "            debug=False, loader_path=None)\n",
    "mt.compile(optimizer=opt, loss=callback.transformer_dist_train_loss)\n",
    "mt.run_eagerly = True\n",
    "# batch = (dataset.generators_dict[\"train\"][0])\n",
    "# print(type(dataset.generators_dict[\"train\"]))\n",
    "# mt.train_on_batch(batch[0], batch[1])\n",
    "print(len(dataset.generators_dict[\"train\"][0]))\n",
    "print(len(dataset.generators_dict[\"train\"][1]))\n",
    "print(len(dataset.generators_dict[\"train\"][2]))\n",
    "# ds = tf.data.Dataset.from_generator(dataset.generators_dict[\"train\"].__iter__(), output_types=tf.float32)\n",
    "mt.fit(x=dataset.generators_dict[\"train\"][0], y=dataset.generators_dict[\"train\"][1], epochs=EPOCHS, callbacks=callbacks_list)\n",
    "# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\n",
    "# mt.fit(dataset.generators_dict[\"train\"], epochs=EPOCHS, callbacks=callbacks_list)\n",
    "\n",
    "# define tensorboard writer\n",
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "train_log_dir = 'logs/mt_decoder/'+current_time+'/train'\n",
    "eval_log_dir = 'logs/mt_decoder/'+current_time+'/eval'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "eval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\n",
    "EPOCHS=2\n",
    "\n",
    "# Train Start\n",
    "# history = mt.fit(dataset.slide_seq2seq_batch(batch_size, max_seq), epochs=EPOCHS)\n",
    "\n",
    "# idx = 0\n",
    "# for e in range(epochs):\n",
    "#     mt.reset_metrics()\n",
    "#     for b in range(len(dataset.files) // batch_size):\n",
    "#         try:\n",
    "#             batch_x, batch_y = dataset.slide_seq2seq_batch(batch_size, max_seq)\n",
    "#         except:\n",
    "#             continue\n",
    "#         result_metrics = mt.train_on_batch(batch_x, batch_y)\n",
    "#         if b % 100 == 0:\n",
    "#             eval_x, eval_y = dataset.slide_seq2seq_batch(batch_size, max_seq, 'eval')\n",
    "#             eval_result_metrics, weights = mt.evaluate(eval_x, eval_y)\n",
    "#             mt.save(save_path)\n",
    "#             with train_summary_writer.as_default():\n",
    "#                 if b == 0:\n",
    "#                     tf.summary.histogram(\"target_analysis\", batch_y, step=e)\n",
    "#                     tf.summary.histogram(\"source_analysis\", batch_x, step=e)\n",
    "#\n",
    "#                 tf.summary.scalar('loss', result_metrics[0], step=idx)\n",
    "#                 tf.summary.scalar('accuracy', result_metrics[1], step=idx)\n",
    "#\n",
    "#             with eval_summary_writer.as_default():\n",
    "#                 if b == 0:\n",
    "#                     mt.sanity_check(eval_x, eval_y, step=e)\n",
    "#\n",
    "#                 tf.summary.scalar('loss', eval_result_metrics[0], step=idx)\n",
    "#                 tf.summary.scalar('accuracy', eval_result_metrics[1], step=idx)\n",
    "#                 for i, weight in enumerate(weights):\n",
    "#                     with tf.name_scope(\"layer_%d\" % i):\n",
    "#                         with tf.name_scope(\"w\"):\n",
    "#                             utils.attention_image_summary(weight, step=idx)\n",
    "#                 # for i, weight in enumerate(weights):\n",
    "#                 #     with tf.name_scope(\"layer_%d\" % i):\n",
    "#                 #         with tf.name_scope(\"_w0\"):\n",
    "#                 #             utils.attention_image_summary(weight[0])\n",
    "#                 #         with tf.name_scope(\"_w1\"):\n",
    "#                 #             utils.attention_image_summary(weight[1])\n",
    "#             idx += 1\n",
    "#             print('\\n====================================================')\n",
    "#             print('Epoch/Batch: {}/{}'.format(e, b))\n",
    "#             print('Train >>>> Loss: {:6.6}, Accuracy: {}'.format(result_metrics[0], result_metrics[1]))\n",
    "#             print('Eval >>>> Loss: {:6.6}, Accuracy: {}'.format(eval_result_metrics[0], eval_result_metrics[1]))\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yf7X6VV8_NyU",
    "executionInfo": {
     "status": "error",
     "timestamp": 1639490690233,
     "user_tz": -60,
     "elapsed": 47581,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    },
    "outputId": "1ac1e2e0-bb97-4b1e-8956-d1eb95730605"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for e in range(epochs):\n",
    "#     for b in range(len(dataset.files) // batch_size):\n",
    "#         batch_x, batch_y = dataset.seq2seq_batch(batch_size, par.max_seq)\n",
    "#         result_metrics = mt.train_on_batch(batch_x, batch_y)\n",
    "#         if b % 1 == 0:\n",
    "#           print('===========================================\\n')\n",
    "#           print('Loss: {:6.6}, Accuracy: {:3.2}'.format(result_metrics[0], result_metrics[1]))\n",
    "#           mt.save('mt-2048-h4-dim256.h5', )\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GIsCBH0e_NyW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MusicTransformer"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QR77-ehl_NyX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python train.py --epochs=1 --pickle_dir=\"dataset/processed\" --save_path=\"bin/models\" --max_seq=2048  --batch_size=1 --l_r=0.001"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IVOZPogN_NyZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "UQp14qG1_NyZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fill_with_placeholder(prev_data: list, max_len: int, max_val: int=239):\n",
    "    placeholder = [max_val for _ in range(max_len - len(prev_data))]\n",
    "    return placeholder+prev_data"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ERaK3zJq_Nya"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(max_seq):\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "j1dFNr9c_Nyb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# suppress warning/error messages in terminal\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# load in data from mnist dataset (60k training, 10k test)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape and convert to one-hot\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# split main training set into train/validation sets (6k out of 60k data points reserved for validation)\n",
    "x_validate = x_train[:6000, :]\n",
    "y_validate = y_train[:6000, :]\n",
    "x_train = x_train[6000:, :]\n",
    "y_train = y_train[6000:, :]\n",
    "\n",
    "'''\n",
    "Building layers for the feedforward NN:\n",
    "Hidden layers have 56, 96, 96, and 56 nodes, in that order.\n",
    "Takes in 784 values (pixel input) and outputs 10 values (predicted probability for each number, 0-9).\n",
    "'''\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(56,activation='relu',input_shape=(784,)))\n",
    "model.add(layers.Dense(96,activation='relu'))\n",
    "model.add(layers.Dense(96,activation='relu'))\n",
    "model.add(layers.Dense(56,activation='relu'))\n",
    "model.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "\t\t\t  optimizer='adam',\n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "# training the NN\n",
    "epochs = 20\n",
    "history = model.fit(x_train,y_train,epochs=epochs,batch_size=512,verbose=2,validation_data=(x_validate,y_validate))\n",
    "print(\"Finished fitting.\")\n",
    "\n",
    "# plotting learning curves during training (on both training and validation data)\n",
    "epoch_labels = range(1, epochs+1)\n",
    "hist_dict = history.history\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.plot(epoch_labels, hist_dict[\"acc\"],'bo', label=\"Training\")\n",
    "plt.plot(epoch_labels, hist_dict[\"val_acc\"],'go', label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# evaluating final performance using test data\n",
    "print(\"Checking accuracy on test set...\")\n",
    "acc = model.evaluate(x_test,y_test,batch_size=512)\n",
    "print(\"\\nAccuracy on test set: \" + str(acc[1]))\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IkS0hRyn_Nyb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "id": "KjwIFHNWPG5g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1639490622979,
     "user_tz": -60,
     "elapsed": 485,
     "user": {
      "displayName": "Alan Bark",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17441688325191762145"
     }
    }
   },
   "execution_count": 23,
   "outputs": []
  }
 ]
}